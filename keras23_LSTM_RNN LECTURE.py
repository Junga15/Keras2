2021년 1월 6일

2021년 1월 5일 
※참고자료: 1월 5일 갤러리

<RNN 계열_LSTM, Simple RNN, GRU>
#연관키워드: 역전파
#1. LSTM 
#keras23(1~3),26(LSTM),27(LSTM_DNN)
#keras23_LSTM1 필기내용
#LSTM의 구조:행,열, 몇개씩 자르는지,여기도 행무시,인풋쉐이프는 2차원(열과 몇개씩 자르는지가 들어감)
#LSTM의 내부 4개 게이트를 지날 때의 활성화함수: 시그모이드 및 쌍곡탄젠트 함수, 최종 활성화함수:탄젠트함수(tanh,디폴트값:값안주면 탄젠트함수,렐루로 바뀌면 바뀜)
#게이트:순서대로 망각,인풋,셀,아웃풋 게이트 4개->얘네들의 활성화함수 시그모이드 3개,탄젠트 함수1개=>최종 활성화함수:탄젠트함수
#탄젠트 -1~1(최소~최대)
#회귀모델의 아웃풋의 활성화 함수:리니어, 이진분류의 아웃풋레이어의 활성화함수: 시그모이드(중간에도 삽입 가능), 다중분류는 소프트맥스
#(4,3,1): RNN셀에 입력되는 텐서의 모양,아래 3차원 데이터 구성에서(왼쪽부터~)
#batch_size: 4(행)
#timesteps: 인풋시퀀스의 개수 3(열)
#input_dim 각 인풋 시퀀스의 차원 1(몇개씩 자르는지) =>피쳐의 차원수와 비슷한 역할
#※케라스와 함께하는 쉬운 딥러닝(15)-순환형신경망(RNN)기초

#1.2 LSTM 모델에서의 데이터 재구성 =>reshape
#한개씩 잘라서 작업하기 위해 쉐이프를 바꾼다. LSTM레이어를 쓰려면 3차원 데이터야함
# ※Dense는 데이터 2차원(인풋딤은 행무시하고 1차원),LSTM(RNN계열,시계열데이터)은 3차원(인풋딤은 행무시하고 2차원),CNN은 4차원
#LSTM이라고 하면 RNN으로 알아들으면 됨,시계열 데이터->자연어처리,챗봇 시용
#LSTM(시계열 데이터)은 y값이 없다.=>3개의 칼럼으로 y값을 예측함 (ex.삼성전자 주가예측), 우리가 y데이터를 만드는 것(아예 없는 것은 아님)
x=x.reshape(4,3,1) #[[[1],[2],[3]],[[2],[3],[4]]...] #원소는 똑같음, 데이터 자체의 손실은 없음,데이터의 갯수가 변화되면 안됨
                                                     #총 곱한 값은 같아야 함 ex>(5,4),(5,4,1),(5,2,2) =>예를 들어 2개씩 자른다면 열은 2가 되어야함
#-1은 행렬이든 넘파이배열이든,판다스 배열이든 제일 끝에 있는 숫자를 가리킴

model.summary() #질문:Param#이 왜 480인가? =>결과값 나올때까지 오래걸림#LSTM 모델구조를 알아야
                #답: 4(LSTM의 4개의 게이트)X([인풋노드1+바이어스1+아웃풋노드10])X10(한바퀴돌기 때문에 아웃풋노드,역전파의 개념과 유사함)=480개
                #   =[(4X1+4X1+4X10)]X10도 가능=>[]LSTM의 상층게이트


#2. Simple RNN 

#3. GRU 

#역전파
인풋에 x값이 2개,은닉층1개(은닉층노드2개),아웃풋에 y값이 2개(노드2개)인 모델
y1,y2=w1x1+w2x2+..+b : 다층 퍼셉트론 모델
처음의 웨이트(가중치값)는 랜덤하게 들어감. (물론 파라미터갯수만큼 가중치가 들어감)
처음에 들어갈 때 한번 에포크 돌며 아웃풋y값을 구하고 거꾸로 다시 돌아가서(2번째 에포크부터)
아웃풋값 y'에서 은닉층에 돌아가서 다시 웨이트값을 수정하며 최적의 웨이트(가중치)를 구함.
거꾸로 가기 때문에 역전파